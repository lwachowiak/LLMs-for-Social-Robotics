# Are Large Language Models Aligned with People's Social Intuitions for Humanâ€“Robot Interactions?

In this paper, we investigate the alignment between LLMs and people in experiments from social HRI.

<img src="https://github.com/lwachowiak/LLMs-for-Social-Robotics/blob/main/overview.png" width="600" />


## Results 
Correlations are highest with GPT-4, as shown in the following scatterplots:

**Experiment 1**
![Correlations for Exp1 with GPT-4](https://github.com/lwachowiak/LLMs-for-Social-Robotics/blob/main/code/Experiment_1/plots/gpt_4_correlations.png)

**Experiment 2**
![Correlations for Exp1 with GPT-4](https://github.com/lwachowiak/LLMs-for-Social-Robotics/blob/main/code/Experiment_2/plots/gpt_4_avg_correlations.png)
![Correlations for Exp1 with GPT-4](https://github.com/lwachowiak/LLMs-for-Social-Robotics/blob/main/code/Experiment_2/plots/gpt_4_diff_correlations.png)


For full results, refer to the paper. Scatterplots for other models can be found [here](https://github.com/lwachowiak/LLMs-for-Social-Robotics/tree/main/code/Experiment_1/plots) for Experiment 1 and [here](https://github.com/lwachowiak/LLMs-for-Social-Robotics/tree/main/code/Experiment_2/plots) for Experiment 2. 
